{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import f1_score, roc_curve, roc_auc_score, precision_recall_curve, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sys.path.insert(0, \"/mnt/Dados/Documentos/xgboost/python-package/\")\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif,chi2\n",
    "from sklearn.preprocessing import Binarizer, scale, StandardScaler, minmax_scale\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.model_selection import GroupKFold, LeavePGroupsOut, LeaveOneGroupOut\n",
    "\n",
    "def shuffled(array):\n",
    "    x = array.values.copy()\n",
    "    np.random.shuffle(x)\n",
    "    return x\n",
    "\n",
    "def shuffled2(array):\n",
    "    x = array.copy()\n",
    "    np.random.shuffle(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 8.38 s, total: 1min 51s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read database from disk\n",
    "data = pd.read_csv(\"../IC2017_DATA/augmented_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "# data = data.loc[shuffled(data.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ExG_contrast_np.pi/4', 'ExG_contrast_0', 'ExG_contrast_3*np.pi/2',\n",
       "       'ExG_contrast_7*np.pi/4', 'ExG_correlation_np.pi/4',\n",
       "       'ExG_correlation_0', 'ExG_correlation_3*np.pi/2',\n",
       "       'ExG_correlation_7*np.pi/4', 'ExG_energy_np.pi/4', 'ExG_energy_0',\n",
       "       ...\n",
       "       'interior_61', 'interior_62', 'interior_63', 'target', 'img_num',\n",
       "       'noise_num', 'rot_num', 'sh_num', 'block_num', 'base_num'],\n",
       "      dtype='object', length=321)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['target']\n",
    "IMG = data['img_num']\n",
    "BLOCK = data['block_num']\n",
    "solo = data['base_num']\n",
    "\n",
    "NOISE = data['noise_num']\n",
    "ROT = data['rot_num']\n",
    "SHIFT = data['sh_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data['target']\n",
    "# del data['img_num']\n",
    "# del data['block_num']\n",
    "# del data['base_num']\n",
    "\n",
    "# del data['noise_num']\n",
    "# del data['rot_num']\n",
    "# del data['sh_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['target', 'img_num', 'block_num', 'base_num', 'noise_num', 'rot_num', 'sh_num'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL = (NOISE == 0) & (ROT == 0) & (SHIFT == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0)  AUC and Mean Acc. Analysis of everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestTreshold(FPR, TPR, TH):\n",
    "    \"\"\"This function is for calculating the Mean Accuracy, given a ROC curve\"\"\"\n",
    "    i_max = max(range(len(TPR)), key = lambda x : TPR[x] + 1 - FPR[x])\n",
    "    mean_accuracy = (TPR[i_max] + 1 - FPR[i_max])/2\n",
    "    return mean_accuracy, TH[i_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPerformance(true, prediction):\n",
    "    auc = roc_auc_score(true, prediction)        \n",
    "    fpr, tpr, th = roc_curve(true, prediction)\n",
    "    if auc < .5:\n",
    "        fpr, tpr = tpr, fpr\n",
    "        auc = 1 - auc\n",
    "    mean_acc, bestTH = getBestTreshold(fpr, tpr, th)\n",
    "    return auc, mean_acc, bestTH, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByImages(data, test_size, repetitions):\n",
    "    images = list(set(IMG.loc[data.index]))\n",
    "    n = len(images)\n",
    "    for _ in range(repetitions):\n",
    "        np.random.shuffle(images)\n",
    "        split_point = int(n * (1-test_size))\n",
    "        yield images[:split_point], images[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBTrain(data, train_imgs, val_imgs, y):\n",
    "\n",
    "    X_train, y_train = data.loc[IMG.isin(train_imgs)], y.loc[IMG.isin(train_imgs)]\n",
    "    X_val, y_val = data.loc[IMG.isin(val_imgs)], y.loc[IMG.isin(val_imgs)]\n",
    "    # print(len(train_imgs), len(val_imgs))\n",
    "    \n",
    "    ratio = float(np.sum(y_train == 1)) / np.sum(y_train==0)\n",
    "\n",
    "    clf = xgb.XGBClassifier(\n",
    "                    max_depth = 4,\n",
    "                    n_estimators=1000,\n",
    "                    learning_rate=0.2, \n",
    "                    nthread=6,\n",
    "                    subsample=1.0,\n",
    "                    colsample_bytree=1,\n",
    "                    scale_pos_weight = ratio,\n",
    "                    reg_alpha=0.03,\n",
    "                    seed=1301)\n",
    "\n",
    "    clf.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"auc\",\n",
    "            eval_set=[(X_val, y_val)], verbose = False)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LessCompact = [0]\n",
    "MoreCompact = [1]\n",
    "Calibrated = [0, 1]\n",
    "NotCalibrated = [2]\n",
    "experiments = [[LessCompact, MoreCompact], [MoreCompact, LessCompact],\n",
    "              [Calibrated, NotCalibrated], [NotCalibrated, Calibrated]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('AccTest_augmented_results.txt', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment [0] [1]\n",
      " > 26 iterations \t Val vs Test (MAc) = 0.948 vs 0.621\n",
      " > 4 iterations \t Val vs Test (MAc) = 0.976 vs 0.804\n",
      " > 42 iterations \t Val vs Test (MAc) = 0.960 vs 0.850\n",
      " > 35 iterations \t Val vs Test (MAc) = 0.984 vs 0.864\n",
      " > 25 iterations \t Val vs Test (MAc) = 0.969 vs 0.652\n",
      "Experiment [1] [0]\n",
      " > 39 iterations \t Val vs Test (MAc) = 0.925 vs 0.954\n",
      " > 113 iterations \t Val vs Test (MAc) = 0.932 vs 0.947\n",
      " > 31 iterations \t Val vs Test (MAc) = 0.917 vs 0.955\n",
      " > 38 iterations \t Val vs Test (MAc) = 0.933 vs 0.956\n",
      " > 23 iterations \t Val vs Test (MAc) = 0.926 vs 0.957\n",
      "Experiment [0, 1] [2]\n",
      " > 12 iterations \t Val vs Test (MAc) = 0.925 vs 0.722\n",
      " > 13 iterations \t Val vs Test (MAc) = 0.922 vs 0.715\n",
      " > 29 iterations \t Val vs Test (MAc) = 0.925 vs 0.718\n",
      " > 33 iterations \t Val vs Test (MAc) = 0.922 vs 0.716\n",
      " > 26 iterations \t Val vs Test (MAc) = 0.902 vs 0.708\n",
      "Experiment [2] [0, 1]\n",
      " > 12 iterations \t Val vs Test (MAc) = 0.895 vs 0.866\n",
      " > 3 iterations \t Val vs Test (MAc) = 0.968 vs 0.898\n",
      " > 5 iterations \t Val vs Test (MAc) = 0.801 vs 0.906\n",
      " > 46 iterations \t Val vs Test (MAc) = 0.856 vs 0.915\n",
      " > 19 iterations \t Val vs Test (MAc) = 0.855 vs 0.890\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "n_reps = 5\n",
    "auc_matrix = np.zeros((4, n_reps))\n",
    "mac_matrix = np.zeros((4, n_reps))\n",
    "auc_matrix_val = np.zeros((4, n_reps))\n",
    "clfs = [[None] * n_reps] * 4 # 4 is the number of experiments\n",
    "\n",
    "for e, (left, test) in enumerate(experiments):\n",
    "    print(\"Experiment\", left, test)\n",
    "    print(\"Experiment\", left, test, file = file)\n",
    "    Xleft = data.loc[solo.isin(left)]\n",
    "    Xtest = data.loc[solo.isin(test)]\n",
    "    # print(len(Xleft), len(Xtest))\n",
    "    \n",
    "    \n",
    "    for i, (train_imgs, val_imgs) in enumerate(splitByImages(Xleft, 1/4, n_reps)):\n",
    "        # train\n",
    "        clf = XGBTrain(data, train_imgs, val_imgs, y)\n",
    "        \n",
    "        # get val performance of this trained model: ROC, AUC and MAc, and also the best threshold value for test\n",
    "        mask = IMG.isin(val_imgs) & ORIGINAL\n",
    "        prediction = clf.predict_proba(data.loc[mask])[:,-1]\n",
    "        val_auc, val_mean_acc, bestTH, fpr, tpr = getPerformance(y.loc[mask], prediction)\n",
    "        \n",
    "        # get train either?\n",
    "        \n",
    "        # get test performance\n",
    "        mask = solo.isin(test) & ORIGINAL\n",
    "        prediction = clf.predict_proba(data.loc[mask])[:,-1]\n",
    "        mean_acc = accuracy_score(y.loc[mask], 1 * (prediction > bestTH)) # 1 * array = array.astype(int)\n",
    "        \n",
    "               \n",
    "        # save performance\n",
    "        mac_matrix[e, i] = mean_acc\n",
    "        auc_matrix_val[e, i] = clf.best_score\n",
    "        \n",
    "        # save classifiers for further analysis\n",
    "#         clfs[e][i] = clf\n",
    "        \n",
    "        print(\" > %d iterations \\t Val vs Test (MAc) = %.3f vs %.3f\" % (clf.best_iteration, \n",
    "                                                                               val_mean_acc, mean_acc))\n",
    "        print(\" > %d iterations \\t Val vs Test (MAc) = %.3f vs %.3f\" % (clf.best_iteration, \n",
    "                                                                               val_mean_acc, mean_acc), file = file)\n",
    "        \n",
    "print(\"Done.\")  \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, (left, test) in enumerate(experiments):\n",
    "    print(\"Experiment\", left, test)\n",
    "    print(\"Experiment\", left, test, file = file)\n",
    "    Xleft = data.loc[solo.isin(left)]\n",
    "    Xtest = data.loc[solo.isin(test)]\n",
    "    for i, (train_imgs, val_imgs) in enumerate(splitByImages(Xleft, 1/4, n_reps)):\n",
    "        \n",
    "        print(\" > %d iterations \\t Val vs Test (MAc) = %.3f vs %.3f\" % (clf.best_iteration, \n",
    "                                                                               val_mean_acc, mean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for each soil for all columns\n",
    "# and save AUC and MAc for each soil for all columns in two separate dataframes\n",
    "\n",
    "scores_auc = pd.DataFrame(None, columns = [0, 1, 2], index=data.columns)\n",
    "scores_mac = pd.DataFrame(None, columns = [0, 1, 2], index=data.columns)\n",
    "for soil in range(3):\n",
    "    plt.figure(figsize=(12,9))\n",
    "    print(\"\\nsolo:\", soil)\n",
    "    \n",
    "    for col in data.columns:\n",
    "        score = roc_auc_score(y.loc[solo == soil], data.loc[solo == soil][col])\n",
    "        fpr, tpr, th = roc_curve(y.loc[solo == soil], data.loc[solo == soil][col])\n",
    "        if score < .5:\n",
    "            fpr, tpr = tpr, fpr\n",
    "            score = 1 - score\n",
    "        mean_acc, _ = getBestTreshold(fpr, tpr, th)\n",
    "\n",
    "        scores_auc.loc[col, soil] = score\n",
    "        scores_mac.loc[col, soil] = mean_acc\n",
    "\n",
    "        #print(\" > %-30s AUC = %.3f \\t MeanAcc = %.3f\" % (col, score, mean_acc))\n",
    "        plt.plot(fpr, tpr)\n",
    "\n",
    "    plt.ylim((0,1))\n",
    "    plt.xlim((0,1))\n",
    "    plt.plot([0,1], [0,1], \"k--\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mac[\"amean\"] = scores_mac[[0,1,2]].mean(axis=1)\n",
    "scores_mac[\"min\"] = scores_mac[[0,1,2]].min(axis=1)\n",
    "#scores_mac[\"gmean\"] = np.power(scores_mac[[0,1,2]].prod(axis=1) , 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = scores_mac.sort_values(\"min\", ascending=False)[[0,1,2]].iloc[:10]\n",
    "pyperclip.copy(best_features.to_latex())\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = scores_mac.sort_values(\"min\", ascending=False)[[0,1,2]].iloc[:10].index\n",
    "plt.figure(figsize=(16,5))\n",
    "for soil in range(3):\n",
    "    plt.subplot(1,3,soil+1)\n",
    "    \n",
    "    for col in cols:\n",
    "        score = roc_auc_score(y.loc[solo == soil], data.loc[solo == soil][col])\n",
    "        fpr, tpr, th = roc_curve(y.loc[solo == soil], data.loc[solo == soil][col])\n",
    "        if score < .5:\n",
    "            fpr, tpr = tpr, fpr\n",
    "            score = 1 - score\n",
    "            \n",
    "        #print(\" > %-30s AUC = %.3f \\t MeanAcc = %.3f\" % (col, score, mean_acc))\n",
    "        plt.plot(fpr, tpr, label = col)\n",
    "    plt.title(\"Solo %d\" % soil)\n",
    "    plt.ylim((0,1))\n",
    "    plt.xlim((0,1))\n",
    "    plt.plot([0,1], [0,1], \"k--\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.legend(loc = 4)\n",
    "plt.savefig(\"report1/figures/10bestminfeatures.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mac.sort_values(2, ascending=False)[2].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Check VI discriminant capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, we check the AUC, Mean Accuracy and ROC curves for each VI in each soil type (0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_VI = [\"ExG\", \"ExGR\", \"CIVE\", \"VEG\", \"WI\", \"NGRDI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VI_cols = [name + \"_mean\" for name in names_VI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only columns with mean of VI values of 16x16 regions\n",
    "VI_data = data[[name + \"_mean\" for name in names_VI]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for each generalization test:\n",
    "    separate into test and rest (how many times?)\n",
    "    separate rest into train and val (how many times?)\n",
    "    find best th value and best VI (which mean: keep the VI used and the TH.\n",
    "        > maybe save all in a table and then order it\n",
    "    test the best\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_matrix = np.zeros((4, reps))\n",
    "VI_scores = [{name + \"_mean\" : None for name in names_VI}] * 4 # 4 is the number of experiments\n",
    "\n",
    "for e, (left, test) in enumerate(experiments):\n",
    "    print(\"Experiment\", left, test)\n",
    "    Xleft = data.loc[solo.isin(left)]\n",
    "    Xtest = data.loc[solo.isin(test)]\n",
    "    \n",
    "    \n",
    "    for col in VI_cols:\n",
    "        \n",
    "        # get val performance of this trained model: ROC, AUC and MAc, and also the best threshold value for test\n",
    "        prediction = Xleft[col]\n",
    "        val_auc, val_mean_acc, bestTH, fpr, tpr = getPerformance(y.loc[solo.isin(left)], prediction)\n",
    "\n",
    "        \n",
    "        # get test performance\n",
    "        prediction = Xtest[col]\n",
    "        mean_acc = accuracy_score(y.loc[solo.isin(test)], 1 * (prediction > bestTH)) # 1 * array = array.astype(int)\n",
    "        if col == \"CIVE_mean\":\n",
    "            mean_acc = accuracy_score(y.loc[solo.isin(test)], 1 * (prediction < bestTH))\n",
    "        \n",
    "               \n",
    "        # save performance\n",
    "        VI_scores[e][col] = mean_acc\n",
    "        \n",
    "        print(\" > %s \\t Val vs Test (MAc) = %.3f vs %.3f\" % (col, val_mean_acc, mean_acc))\n",
    "        \n",
    "print(\"Done.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 5\n",
    "auc_matrix = np.zeros((4, reps))\n",
    "mac_matrix = np.zeros((4, reps))\n",
    "auc_matrix_val = np.zeros((4, reps))\n",
    "clfs = [[None] * n_reps] * 4 # 4 is the number of experiments\n",
    "\n",
    "for e, (left, test) in enumerate(experiments):\n",
    "    print(\"Experiment\", left, test)\n",
    "    Xleft = data.loc[solo.isin(left)]\n",
    "    Xtest = data.loc[solo.isin(test)]\n",
    "    \n",
    "    \n",
    "    for i, (train_imgs, val_imgs) in enumerate(splitByImages(Xleft, 1/4, n_reps)):\n",
    "        # train\n",
    "        clf = XGBTrain(data, train_imgs, val_imgs, y)\n",
    "        \n",
    "        # get val performance of this trained model: ROC, AUC and MAc, and also the best threshold value for test\n",
    "        prediction = clf.predict_proba(data.loc[IMG.isin(val_imgs)])[:,-1]\n",
    "        val_auc, val_mean_acc, bestTH, fpr, tpr = getPerformance(y.loc[IMG.isin(val_imgs)], prediction)\n",
    "        \n",
    "        # get train either?\n",
    "        \n",
    "        # get test performance\n",
    "        prediction = clf.predict_proba(data.loc[solo.isin(test)])[:,-1]\n",
    "        mean_acc = accuracy_score(y.loc[solo.isin(test)], 1 * (prediction > bestTH)) # 1 * array = array.astype(int)\n",
    "        \n",
    "               \n",
    "        # save performance\n",
    "        mac_matrix[e, i] = mean_acc\n",
    "        auc_matrix_val[e, i] = clf.best_score\n",
    "        \n",
    "        # save classifiers for further analysis\n",
    "        clfs[e][i] = clfs\n",
    "        \n",
    "        print(\" > %d iterations \\t Val vs Test (MAc) = %.3f vs %.3f\" % (clf.best_iteration, \n",
    "                                                                               val_mean_acc, mean_acc))\n",
    "        \n",
    "print(\"Done.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise do Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ind = 2 # base 2 out is better to find out why training in 0 and 1 fail in generalizing to 2\n",
    "clf = clfs[clf_ind][0]\n",
    "\n",
    "train_index, test_index = splits[clf_ind]\n",
    "X_train, y_train = data.iloc[train_index], y.iloc[train_index]\n",
    "X_test, y_test = data.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "predict = clf.predict_proba(X_test, ntree_limit=clf.best_iteration)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "for col in scores_mac[2].sort_values(ascending=False).index[:10]:\n",
    "    score = roc_auc_score(y.loc[solo==2], data.loc[solo==2][col])\n",
    "    fpr, tpr, th = roc_curve(y.loc[solo==2], data.loc[solo==2][col])\n",
    "    if score < .5:\n",
    "        fpr, tpr = tpr, fpr\n",
    "        score = 1 - score\n",
    "    mean_acc, _ = getBestTreshold(fpr, tpr, th)\n",
    "    print(\" > %-30s AUC = %.3f \\t MeanAcc = %.3f\" % (col, score, mean_acc))\n",
    "    plt.plot(fpr, tpr, label = col)\n",
    "\n",
    "score = roc_auc_score(y_test, predict)   \n",
    "FPR, TPR, TH = roc_curve(y_test, predict)\n",
    "mean_acc, _ = getBestTreshold(FPR, TPR, TH)\n",
    "print(\" > %-30s AUC = %.3f \\t MeanAcc = %.3f\" % (\"predict\", score, mean_acc))\n",
    "plt.plot(FPR,TPR, \"k-\", label = \"predict\", lw = 1.5)\n",
    "\n",
    "plt.title(\"Top 10 features with higher minimum Mean Accuracy in all three bases vs Learning\")\n",
    "plt.ylim((0,1))\n",
    "plt.xlim((0,1))\n",
    "plt.plot([0,1], [0,1], \"k--\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.legend();\n",
    "plt.savefig(\"report1/figures/prediction_vs_bestFeatures.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "for col in importance_single[2].sort_values(ascending=False)[:10].index:\n",
    "    score = roc_auc_score(y.loc[solo==2], data.loc[solo==2][col])\n",
    "    fpr, tpr, th = roc_curve(y.loc[solo==2], data.loc[solo==2][col])\n",
    "    if score < .5:\n",
    "        fpr, tpr = tpr, fpr\n",
    "        score = 1 - score\n",
    "    mean_acc, _ = getBestTreshold(fpr, tpr, th)\n",
    "    print(\" > %-30s AUC = %.3f \\t MeanAcc = %.3f\" % (col, score, mean_acc))\n",
    "    plt.plot(fpr, tpr, label = col)\n",
    "\n",
    "score = roc_auc_score(y_test, predict)   \n",
    "FPR, TPR, TH = roc_curve(y_test, predict)\n",
    "mean_acc, _ = getBestTreshold(FPR, TPR, TH)\n",
    "print(\" > %-30s AUC = %.3f \\t MeanAcc = %.3f\" % (\"predict\", score, mean_acc))\n",
    "plt.plot(FPR,TPR, \"k-\", label = \"predict\", lw = 1.5)\n",
    "\n",
    "plt.title(\"Top 10 features with higher minimum importace value in all three bases vs Learning\")\n",
    "plt.ylim((0,1))\n",
    "plt.xlim((0,1))\n",
    "plt.plot([0,1], [0,1], \"k--\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.legend()\n",
    "plt.savefig(\"report1/figures/10bestminGAIN_vs_predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.scatter(predict, y_test, alpha = .1)\n",
    "th = TH[th_ix]\n",
    "plt.plot([th, th], [1,0], \"r-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = clf.predict_proba(data, ntree_limit=clf.best_iteration)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = ((y == 1) & (train_pred < th))\n",
    "m0 = ((y == 0) & (train_pred > th))\n",
    "m1.sum(), m0.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = dec.fit_transform(StandardScaler().fit_transform(data.fillna(10**-5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "xvr = dec.explained_variance_ratio_[:30]\n",
    "plt.bar(range(len(xvr)), np.cumsum(xvr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for c in {0,1}:\n",
    "    plt.subplot(1,2,c+1)\n",
    "    plt.scatter(pca[y==(1-c),0], pca[y==(1-c),1], alpha = 0 * .2, label = \"classe %d\" % (1-c))\n",
    "    plt.scatter(pca[y==c,0], pca[y==c,1], alpha = .2, label = \"classe %d\" % c, color = \"blue\" if c == 0 else \"yellow\")\n",
    "    plt.plot([0,0], [pca[:,1].min(), pca[:,1].max()], \"r--\")\n",
    "    plt.plot([pca[:,0].min(), pca[:,0].max()], [0,0], \"r--\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "for c in {0,1}:\n",
    "    plt.scatter(pca[y==c,0], pca[y==c,1], alpha = .2, label = \"classe %d\" % c)\n",
    "plt.scatter(pca[m0,0], pca[m0,1], label = \"miss0\")\n",
    "plt.scatter(pca[m1,0], pca[m1,1], label = \"miss1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util.shape import view_as_blocks\n",
    "from skimage.io import imread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"imgs_orig\", \"gts_orig\"] # TROCAR AQUI DEPENDENDO DA BASE\n",
    "\n",
    "path_imgs = sorted([ paths[0] + '/' + i for i in listdir(paths[0]) ])\n",
    "path_gts = sorted([ paths[1] + '/' + i for i in listdir(paths[1]) ])\n",
    "\n",
    "\n",
    "paths2 = [\"imgs\", \"gts\"] # TROCAR AQUI DEPENDENDO DA BASE\n",
    "\n",
    "path_imgs2 = sorted([ paths2[0] + '/' + i for i in listdir(paths2[0]) ])\n",
    "path_gts2 = sorted([ paths2[1] + '/' + i for i in listdir(paths2[1]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misses = pd.concat([IMG[m0].value_counts().rename(\"NOVEG-miss\"), IMG[m1].value_counts().rename(\"VEG-miss\")], axis = 1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misses[\"sum\"] = misses[\"NOVEG-miss\"] + misses[\"VEG-miss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "misses.sort_values(\"sum\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(ind, color):\n",
    "    x_ini = (ind % 32) * 16\n",
    "    x_end = x_ini + 15\n",
    "    y_ini = (ind // 32) * 16\n",
    "    y_end = y_ini + 15\n",
    "    plt.fill_between([x_ini, x_end], y_ini, y_end, alpha =.4, color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 41\n",
    "if i >= 40:\n",
    "    img = imread(path_imgs2[i-40])\n",
    "    gt = imread(path_gts2[i-40], as_grey=True)\n",
    "else:\n",
    "    img = imread(path_imgs[i])\n",
    "    gt = imread(path_gts[i], as_grey=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "for b in BLOCK[(IMG == i) & m1]:\n",
    "    highlight(b, \"red\")\n",
    "for b in BLOCK[(IMG == i) & m0]:\n",
    "    highlight(b, \"blue\")\n",
    "plt.ylim((511,0))\n",
    "plt.xlim((0,511))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(gt)\n",
    "for b in BLOCK[(IMG == i) & (y == 1)]:\n",
    "    highlight(b, \"green\")\n",
    "plt.ylim((511,0))\n",
    "plt.xlim((0,511))\n",
    "\n",
    "plt.savefig(\"report1/figures/errovisu%02d.png\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
